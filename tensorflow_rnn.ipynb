{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN language model, based on Mikolov and tensorflow\n",
    "\n",
    "## Still to be done:\n",
    "\n",
    "* Integrating a word embedding (such as CBOW) as a preprocessing layer before applying the RNN.\n",
    "* Adding the evaluation part of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.models.rnn.ptb import reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### global parameters\n",
    "\n",
    "* __num_steps__:  length of a sequence\n",
    "* __btt_steps__:  length of the truncated backpropagation\n",
    "* __num_epochs__: number of randomized epochs over the whole dataset\n",
    "* __state_size__: number of hidden units in the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 21\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "embedding_size = 99\n",
    "batch_size = 83\n",
    "state_size = 77\n",
    "\n",
    "num_negative_samples = 40\n",
    "\n",
    "dt = tf.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load example data\n",
    "\n",
    "This data set is the original one used by Mikolov in his PhD thesis. You can find in the __ptb_data__ folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    '''\n",
    "        Returns the Penn Tree Bank corpus as list of indices, each representing a unique words type in the corpus.\n",
    "    '''\n",
    "    raw_data = reader.ptb_raw_data('ptb_data')\n",
    "    return tuple(map(np.array, raw_data[:3])), raw_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reindex_sorted(data):\n",
    "    unique, counts = np.unique(data, return_counts=True)\n",
    "    index = unique[np.argsort(counts)[::-1]]\n",
    "    m = { index[i]: i for i in range(len(unique))  }\n",
    "    m_inv = { i: index[i] for i in range(len(unique))  }\n",
    "    transform = np.vectorize(lambda x: m[x], otypes=[np.int32])\n",
    "    return transform(data), m, m_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(train_data, valid_data, test_data), vocab_size = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reindex_sorted(np.array([2,4,5,5,5,5,56,6,6,6,6,6,6,6,6,6,6,6,6,6,6,4,3,2,3,4,2,2,2,3,4,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For quick testing it might be useful to truncate the data set $\\Rightarrow$ faster convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_truncate = 100000000\n",
    "train_data = train_data[:n_truncate]\n",
    "valid_data = train_data[:n_truncate]\n",
    "test_data = train_data[:n_truncate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a batch / epoch\n",
    "\n",
    "These two functions generate the epochs of batches for the training. Both implement the iterator pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(data, vocab_size, batch_size, num_steps):\n",
    "    '''\n",
    "        Generates a mini batch of token sequences. \n",
    "        \n",
    "            data:           input data (list of token indices)\n",
    "            vocab_size:     size of the input vocabulary (number of word types)\n",
    "            batch_size:     size of a mini batch\n",
    "            num_steps:      length of a token sequence\n",
    "    '''\n",
    "    begins = np.random.randint(vocab_size - num_steps - 1, size=batch_size)[:,np.newaxis]\n",
    "    ranges = np.arange(num_steps, dtype=np.int)[np.newaxis,:]\n",
    "    indices = begins + ranges    \n",
    "    return data[indices], data[indices+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_epoch(data, vocab_size, batch_size, num_steps):\n",
    "    num_batches = int(len(data) / batch_size / num_steps)\n",
    "    for j in range(num_batches):\n",
    "        yield generate_batch(data, vocab_size, batch_size, num_steps), num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The tensorflow graph\n",
    "\n",
    "This function builds the actual tensorflow graph computing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_tf_graph(init_embedding, vocab_size, batch_size, num_steps, state_size, num_negative_samples=6):\n",
    "    '''\n",
    "        Computes the TF graph for a RNN language model using truncated backpropagation through time.\n",
    "        \n",
    "            init_embedding:    the initial embedding\n",
    "            vocab_size:        size of the input vocabulary (number of word types)\n",
    "            batch_size:        size of a mini batch\n",
    "            num_steps:         number of steps to backpropagate the error\n",
    "            state_size:        size of the hidden state of the RNN cell\n",
    "    '''\n",
    "    \n",
    "    # placeholders for input and targets\n",
    "    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='output_placeholder')\n",
    "\n",
    "    # init state for the rnn cell\n",
    "    default_init_state = tf.zeros([batch_size, state_size])\n",
    "    init_state = tf.placeholder_with_default(\n",
    "        default_init_state,\n",
    "        [batch_size, state_size], \n",
    "        name='state_placeholder')\n",
    "    \n",
    "    # embed the vector\n",
    "    initial_embedding = tf.constant(init_embedding, name=\"initial_embedding\")\n",
    "    embedding = tf.get_variable(\"embedding\", initializer=initial_embedding, dtype=dt)\n",
    "    embedded_vectors = tf.nn.embedding_lookup(embedding, x)\n",
    "    \n",
    "    # squeeze all inputs into a sequence for the rnn cell\n",
    "    #x_as_list = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(1, btt_steps, x_one_hot)]\n",
    "    x_as_list = [tf.squeeze(i, squeeze_dims=[1]) \n",
    "                 for i in tf.split(1, num_steps, embedded_vectors)]\n",
    "    \n",
    "    # appply the rnn cell to the sequence of input vectors\n",
    "    rnn_in = x_as_list\n",
    "    rnn_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=state_size)\n",
    "    rnn_out, training_state = tf.nn.rnn(rnn_cell, rnn_in, initial_state=init_state)\n",
    "    \n",
    "    # project the output sequence via a softmax layer back to vocabulary space\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [vocab_size, state_size])\n",
    "        b = tf.get_variable('b', [vocab_size], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    losses = tf.nn.nce_loss(\n",
    "        weights=W,\n",
    "        biases=b,\n",
    "        inputs=tf.reshape(rnn_out, [batch_size * num_steps, state_size]),\n",
    "        labels=tf.reshape(y, [batch_size * num_steps, 1]),\n",
    "        num_sampled=num_negative_samples,\n",
    "        num_classes=vocab_size,\n",
    "        remove_accidental_hits = True\n",
    "    )\n",
    "\n",
    "    total_loss = tf.reduce_mean(losses)\n",
    "    \n",
    "    # optimize all free variables with respect to that using Adam optimiziation (faster than SGD or Adagrad)\n",
    "    train_step = tf.train.AdamOptimizer().minimize(total_loss)\n",
    "    \n",
    "    # return nodes of the tf graph which are used for further procesing\n",
    "    return {\n",
    "        'total_loss': total_loss,\n",
    "        'x': x,\n",
    "        'y': y,\n",
    "        'init_state': init_state,\n",
    "        'training_state': training_state,\n",
    "        'train_step': train_step\n",
    "    }  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network for a number of epochs\n",
    "\n",
    "This function trains the network on the given dataset for a number of epochs.\n",
    "\n",
    "__TODO__: currently the learned parameters or not returned ye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_network(init_embedding, num_epochs, data, vocab_size, \n",
    "                  batch_size, num_steps, state_size, num_negative_samples):\n",
    "    '''\n",
    "        Trains the RNN language model and prints the current average loss per sequence.\n",
    "        \n",
    "            num_epochs:     number of iterations over the full corpus\n",
    "            data:           input data (list of token indices)\n",
    "            vocab_size:     size of the input vocabulary (number of word types)\n",
    "            batch_size:     size of a mini batch\n",
    "            num_steps:      length of a token sequence\n",
    "            btt_steps:      number of steps to backpropagate the error\n",
    "            state_size:     size of the hidden state of the RNN cell \n",
    "    '''\n",
    "    \n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "        g = build_tf_graph(init_embedding.astype(np.float32), vocab_size, \n",
    "                           batch_size, num_steps, state_size, num_negative_samples)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        losses = []\n",
    "        for k in range(num_epochs):\n",
    "            epoch = generate_epoch(data, vocab_size, batch_size, num_steps)\n",
    "            _agg_loss = 0\n",
    "            for i, ((_x, _y), num_batches) in enumerate(epoch):\n",
    "                \n",
    "                _state = np.zeros((batch_size, state_size))\n",
    "                _loss, _state, _ = sess.run(\n",
    "                    (\n",
    "                        g['total_loss'], \n",
    "                        g['training_state'],\n",
    "                        g['train_step']\n",
    "                    ),\n",
    "                    feed_dict={\n",
    "                        g['x']: _x,\n",
    "                        g['y']: _y,\n",
    "                        g['init_state']: _state\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                _agg_loss += _loss\n",
    "                losses.append(_agg_loss / (i+1))\n",
    "                \n",
    "                print(\"\\repoch:\",k+1,\"/\",num_epochs, end=\"\")\n",
    "                print(\" batch:\", i+1,\"/\", num_batches,\n",
    "                      \"avg loss\", _agg_loss / (i+1), end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_embedding = np.random.uniform(size=(vocab_size, embedding_size)).astype(np.float32)\n",
    "\n",
    "train_network(init_embedding, num_epochs, train_data, vocab_size, batch_size, \n",
    "              num_steps, state_size, num_negative_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
